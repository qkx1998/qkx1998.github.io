{"meta":{"title":"slow dive","subtitle":null,"description":null,"author":"QKX","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"kaggle-York City Taxi Trip Duration","slug":"kaggle-York-City-Taxi-Trip-Duration","date":"2020-04-28T09:40:24.000Z","updated":"2020-04-28T10:44:10.537Z","comments":true,"path":"2020/04/28/kaggle-York-City-Taxi-Trip-Duration/","link":"","permalink":"http://yoursite.com/2020/04/28/kaggle-York-City-Taxi-Trip-Duration/","excerpt":"","text":"背景：预测纽约市出租车旅行的总行驶时间，数据集来源于纽约市出租车和豪华轿车委员会，其中包括上车时间，地理坐标，乘客人数以及其它几个变量。 1 理解赛题背景，赛题数据，评估指标。2 数据清洗，从缺失，重复值，数据分布等角度清洗数据。3 进行探索性数据分析。4 特征工程，从距离，方向，日期等角度新建特征。5 建模预测，利用gbdt,xgboost,lightgbm等进行模型融合。 思维导图： 数据清洗 12345678import pandas as pdimport numpy as npimport pyecharts as pefrom pyecharts import Bar,Line,Overlap,Gridfrom datetime import datetimetrain &#x3D; pd.read_csv(&#39;C:\\\\Users\\\\18438\\\\Desktop\\\\NYC_TAXI\\\\train.csv&#39;)test &#x3D; pd.read_csv(&#39;C:\\\\Users\\\\18438\\\\Desktop\\\\NYC_TAXI\\\\test.csv&#39;) 提取上车日期中的各种日期信息 123456train[&#39;year&#39;] &#x3D; pd.to_datetime(train[&#39;pickup_datetime&#39;]).dt.yeartrain[&#39;date&#39;] &#x3D; pd.to_datetime(train[&#39;pickup_datetime&#39;]).dt.datetrain[&#39;month&#39;] &#x3D; pd.to_datetime(train[&#39;pickup_datetime&#39;]).dt.monthtrain[&#39;weekday&#39;] &#x3D; pd.to_datetime(train[&#39;pickup_datetime&#39;]).dt.weekdaytrain[&#39;day&#39;] &#x3D; pd.to_datetime(train[&#39;pickup_datetime&#39;]).dt.daytrain[&#39;hour&#39;] &#x3D; pd.to_datetime(train[&#39;pickup_datetime&#39;]).dt.hour 去掉行驶时长数据异常值。观察数据存在异常的行驶时长，如1秒和980小时，所以在这里统一去掉偏离平均数两个标准差的数据。 1234m &#x3D; np.mean(train[&#39;trip_duration&#39;])s &#x3D; np.std(train[&#39;trip_duration&#39;])train &#x3D; train[train[&#39;trip_duration&#39;] &lt;&#x3D; m + 2*s]train &#x3D; train[train[&#39;trip_duration&#39;] &gt;&#x3D; m - 2*s] 去掉超过纽约边界的坐标数据 12345678910city_long_border &#x3D; (-74.03, -73.75)city_lat_border &#x3D; (40.63, 40.85) train &#x3D; train[train[&#39;pickup_longitude&#39;] &lt;&#x3D; -73.75]train &#x3D; train[train[&#39;pickup_longitude&#39;] &gt;&#x3D; -74.03]train &#x3D; train[train[&#39;pickup_latitude&#39;] &lt;&#x3D; 40.85]train &#x3D; train[train[&#39;pickup_latitude&#39;] &gt;&#x3D; 40.63]train &#x3D; train[train[&#39;dropoff_longitude&#39;] &lt;&#x3D; -73.75]train &#x3D; train[train[&#39;dropoff_longitude&#39;] &gt;&#x3D; -74.03]train &#x3D; train[train[&#39;dropoff_latitude&#39;] &lt;&#x3D; 40.85]train &#x3D; train[train[&#39;dropoff_latitude&#39;] &gt;&#x3D; 40.63] 探索性数据分析 1 观察不同时间段下的订单量的变化趋势（在分析中订单量认为可反映拥堵情况）2 观察不同时间段下的订单平均时间的变化趋势（在分析中平均时长认为可反映拥堵情况以及长途，短途单的比重）3 观察不同乘客数量 不同出租车公司以及关于store_and_fwd_flag特征的订单量变化4 对经纬度坐标进行Kmeans聚类，观察分布 对各个时间段下的订单量进行统计 123456789101112count_date &#x3D; train.groupby(&#39;date&#39;)[&#39;id&#39;].count().to_frame()count_month &#x3D; train.groupby(&#39;month&#39;)[&#39;id&#39;].count().to_frame()count_weekday &#x3D; train.groupby(&#39;weekday&#39;)[&#39;id&#39;].count().to_frame()count_day &#x3D; train.groupby(&#39;day&#39;)[&#39;id&#39;].count().to_frame()count_hour&#x3D; train.groupby(&#39;hour&#39;)[&#39;id&#39;].count().to_frame()line &#x3D; Line(&#39;订单数量变化趋势&#39;)line1 &#x3D; Line()line2 &#x3D; Line()line3 &#x3D; Line()line4 &#x3D; Line()line.add(&#39;天数&#39;,count_date.index,count_date[&#39;id&#39;]) 123456789line1.add(&#39;.&#39;,count_month.index,count_month[&#39;id&#39;])line2.add(&#39;.&#39;,count_day.index,count_day[&#39;id&#39;])line3.add(&#39;.&#39;,count_weekday.index,count_weekday[&#39;id&#39;])line4.add(&#39;.&#39;,count_hour.index,count_hour[&#39;id&#39;])grid &#x3D; Grid(height&#x3D;720, width&#x3D;1000)grid.add(line1, grid_bottom&#x3D;&quot;60%&quot;, grid_right&#x3D;&quot;60%&quot;)grid.add(line2,grid_bottom&#x3D;&quot;60%&quot;, grid_left&#x3D;&quot;60%&quot;)grid.add(line3,grid_top&#x3D;&quot;60%&quot;, grid_right&#x3D;&quot;60%&quot;)grid.add(line4,grid_top&#x3D;&quot;60%&quot;, grid_left&#x3D;&quot;60%&quot;) 对不同时间段下的订单平均时间的变化趋势进行分析。 123456789101112dtime_date &#x3D; train.groupby(&#39;date&#39;)[&#39;trip_duration&#39;].mean().to_frame()dtime_month &#x3D; train.groupby(&#39;month&#39;)[&#39;trip_duration&#39;].mean().to_frame()dtime_weekday &#x3D; train.groupby(&#39;weekday&#39;)[&#39;trip_duration&#39;].mean().to_frame()dtime_day &#x3D; train.groupby(&#39;day&#39;)[&#39;trip_duration&#39;].mean().to_frame()dtime_hour &#x3D; train.groupby(&#39;hour&#39;)[&#39;trip_duration&#39;].mean().to_frame()line5 &#x3D; Line(&#39;订单时间变化趋势&#39;)bar &#x3D; Bar()overlap &#x3D; Overlap()line5.add(&#39;柱状图&#39;,dtime_date.index,dtime_date[&#39;trip_duration&#39;])bar.add(&#39;折线图&#39;,dtime_date.index,dtime_date[&#39;trip_duration&#39;])overlap.add(line5)overlap.add(bar) 12345678910111213line6 &#x3D; Line()line7 &#x3D; Line()line8 &#x3D; Line()line9 &#x3D; Line()line6.add(&#39;.&#39;,dtime_month.index,dtime_month[&#39;trip_duration&#39;])line7.add(&#39;.&#39;,dtime_day.index,dtime_day[&#39;trip_duration&#39;])line8.add(&#39;.&#39;,dtime_weekday.index,dtime_weekday[&#39;trip_duration&#39;])line9.add(&#39;.&#39;,dtime_hour.index,dtime_hour[&#39;trip_duration&#39;])grid &#x3D; Grid(height&#x3D;720, width&#x3D;1000)grid.add(line6, grid_bottom&#x3D;&quot;60%&quot;, grid_right&#x3D;&quot;60%&quot;)grid.add(line7,grid_bottom&#x3D;&quot;60%&quot;, grid_left&#x3D;&quot;60%&quot;)grid.add(line8,grid_top&#x3D;&quot;60%&quot;, grid_right&#x3D;&quot;60%&quot;)grid.add(line9,grid_top&#x3D;&quot;60%&quot;, grid_left&#x3D;&quot;60%&quot;) 观察不同乘客数量 不同出租车公司以及关于store_and_fwd_flag特征的订单量变化 123456789pc_td &#x3D; train.groupby(&#39;passenger_count&#39;)[&#39;trip_duration&#39;].mean().to_frame()bar2 &#x3D; Bar(&#39;不同乘客数量的行程时长&#39;)bar2.add(&#39;人数&#39;,pc_td.index,pc_td[&#39;trip_duration&#39;])vi_td &#x3D; train.groupby(&#39;vendor_id&#39;)[&#39;trip_duration&#39;].mean().to_frame()bar3 &#x3D; Bar(&#39;不同出租车公司的行程时长&#39;)bar3.add(&#39;公司&#39;,vi_td.index,vi_td[&#39;trip_duration&#39;])saff_td &#x3D; train.groupby(&#39;store_and_fwd_flag&#39;)[&#39;trip_duration&#39;].mean().to_frame()bar4 &#x3D; Bar(&#39;有无store_and_fwd_flag的行程时长&#39;)bar4.add(&#39;有无store_and_fwd_flag&#39;,saff_td.index,saff_td[&#39;trip_duration&#39;]) 绘制上车点的位置 12345678910111213import matplotlib.pyplot as plt %matplotlib inlinecity_long_border &#x3D; (-74.03, -73.75)city_lat_border &#x3D; (40.63, 40.85)plt.scatter(train[&#39;pickup_longitude&#39;].values[:100000], train[&#39;pickup_latitude&#39;].values[:100000], color&#x3D;&#39;darkblue&#39;, s&#x3D;1, label&#x3D;&#39;train&#39;, alpha&#x3D;0.1)plt.legend(loc&#x3D;0)plt.ylabel(&#39;latitude&#39;)plt.xlabel(&#39;longitude&#39;)plt.ylim(city_lat_border)plt.xlim(city_long_border)plt.show() 绘制下车点的位置 1234567891011city_long_border &#x3D; (-74.03, -73.75)city_lat_border &#x3D; (40.63, 40.85)plt.scatter(train[&#39;dropoff_longitude&#39;].values[:100000], train[&#39;dropoff_latitude&#39;].values[:100000], color&#x3D;&#39;darkgreen&#39;, s&#x3D;1, label&#x3D;&#39;train&#39;, alpha&#x3D;0.1)plt.legend(loc&#x3D;0)plt.ylabel(&#39;latitude&#39;)plt.xlabel(&#39;longitude&#39;)plt.ylim(city_lat_border)plt.xlim(city_long_border)plt.show() 通过kmeans对曼哈顿区的上车下车区域进行聚类观察，可以更直观的理解数据。一共分三步：创建位置堆 1234567891011121314151617181920from sklearn.cluster import MiniBatchKMeanscoords &#x3D; np.vstack((train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]].values, train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]].values))sample_ind &#x3D; np.random.permutation(len(coords))[:500000]kmeans &#x3D; MiniBatchKMeans(n_clusters&#x3D;100, batch_size&#x3D;10000).fit(coords[sample_ind])train.loc[:, &#39;pickup_cluster&#39;] &#x3D; kmeans.predict(train[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]])train.loc[:, &#39;dropoff_cluster&#39;] &#x3D; kmeans.predict(train[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]])test.loc[:, &#39;pickup_cluster&#39;] &#x3D; kmeans.predict(test[[&#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;]])test.loc[:, &#39;dropoff_cluster&#39;] &#x3D; kmeans.predict(test[[&#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;]])plt.scatter(train.pickup_longitude.values[:500000], train.pickup_latitude.values[:500000], s&#x3D;10, lw&#x3D;0, c&#x3D;train.pickup_cluster[:500000].values, cmap&#x3D;&#39;autumn&#39;, alpha&#x3D;0.2)plt.xlim(city_long_border)plt.ylim(city_lat_border)plt.xlabel(&#39;Longitude&#39;)plt.ylabel(&#39;Latitude&#39;)plt.show() 特征工程 1.利用haversine计算两个经纬度点之间的距离 作为新特征2.利用两点之间直线距离公式（取对数）计算两个经纬度点之间的另一种距离表达 作为新特征3.是否为周末 作为新特征4.是否为假期 是否为假期前一天 作为新特征5.删除没用的信息6.独热编码 1234567891011121314151617181920212223def extract_features(df): #利用haversine计算两个经纬度点之间的距离 作为新特征 df[&#39;hdistance&#39;] &#x3D; df.apply(lambda r: haversine.haversine((r[&#39;pickup_latitude&#39;],r[&#39;pickup_longitude&#39;]),(r[&#39;dropoff_latitude&#39;], r[&#39;dropoff_longitude&#39;])), axis&#x3D;1) #利用两点之间直线距离公式计算两个经纬度点之间的另一种距离表达 作为新特征 df[&#39;distance&#39;] &#x3D; np.sqrt(np.power(df[&#39;dropoff_longitude&#39;] - df[&#39;pickup_longitude&#39;], 2) + np.power(df[&#39;dropoff_latitude&#39;] - df[&#39;pickup_latitude&#39;], 2)) #对上式求得的距离取对数 df[&#39;log_distance&#39;] &#x3D; np.log(df[&#39;distance&#39;] + 1) #判断是否是周末 df[&#39;is_weekend&#39;] &#x3D; ((df.pickup_datetime.astype(&#39;datetime64[ns]&#39;).dt.dayofweek) &#x2F;&#x2F; 4 &#x3D;&#x3D; 1).astype(float) #是否为 假期 把一些特定假期的日期通过lambda来筛选 是假期的为1 否则为0 df[&#39;is_holyday&#39;] &#x3D; df.apply(lambda row: 1 if (row[&#39;month&#39;]&#x3D;&#x3D;1 and row[&#39;day&#39;]&#x3D;&#x3D;1) or (row[&#39;month&#39;]&#x3D;&#x3D;7 and row[&#39;day&#39;]&#x3D;&#x3D;4) or (row[&#39;month&#39;]&#x3D;&#x3D;11 and row[&#39;day&#39;]&#x3D;&#x3D;11) or (row[&#39;month&#39;]&#x3D;&#x3D;12 and row[&#39;day&#39;]&#x3D;&#x3D;25) or (row[&#39;month&#39;]&#x3D;&#x3D;1 and row[&#39;day&#39;] &gt;&#x3D; 15 and row[&#39;day&#39;] &lt;&#x3D; 21 and row[&#39;weekday&#39;] &#x3D;&#x3D; 0) or (row[&#39;month&#39;]&#x3D;&#x3D;2 and row[&#39;day&#39;] &gt;&#x3D; 15 and row[&#39;day&#39;] &lt;&#x3D; 21 and row[&#39;weekday&#39;] &#x3D;&#x3D; 0) or (row[&#39;month&#39;]&#x3D;&#x3D;5 and row[&#39;day&#39;] &gt;&#x3D; 25 and row[&#39;day&#39;] &lt;&#x3D; 31 and row[&#39;weekday&#39;] &#x3D;&#x3D; 0) or (row[&#39;month&#39;]&#x3D;&#x3D;9 and row[&#39;day&#39;] &gt;&#x3D; 1 and row[&#39;day&#39;] &lt;&#x3D; 7 and row[&#39;weekday&#39;] &#x3D;&#x3D; 0) or (row[&#39;month&#39;]&#x3D;&#x3D;10 and row[&#39;day&#39;] &gt;&#x3D; 8 and row[&#39;day&#39;] &lt;&#x3D; 14 and row[&#39;weekday&#39;] &#x3D;&#x3D; 0) or (row[&#39;month&#39;]&#x3D;&#x3D;11 and row[&#39;day&#39;] &gt;&#x3D; 22 and row[&#39;day&#39;] &lt;&#x3D; 28 and row[&#39;weekday&#39;] &#x3D;&#x3D; 3) else 0, axis&#x3D;1) #是否为假期前一天 方法同上一步 df[&#39;is_day_before_holyday&#39;] &#x3D; df.apply(lambda row: 1 if (row[&#39;month&#39;]&#x3D;&#x3D;12 and row[&#39;day&#39;]&#x3D;&#x3D;31) or (row[&#39;month&#39;]&#x3D;&#x3D;7 and row[&#39;day&#39;]&#x3D;&#x3D;3) or (row[&#39;month&#39;]&#x3D;&#x3D;11 and row[&#39;day&#39;]&#x3D;&#x3D;10) or (row[&#39;month&#39;]&#x3D;&#x3D;12 and row[&#39;day&#39;]&#x3D;&#x3D;24) or (row[&#39;month&#39;]&#x3D;&#x3D;1 and row[&#39;day&#39;] &gt;&#x3D; 14 and row[&#39;day&#39;] &lt;&#x3D; 20 and row[&#39;weekday&#39;] &#x3D;&#x3D; 6) or (row[&#39;month&#39;]&#x3D;&#x3D;2 and row[&#39;day&#39;] &gt;&#x3D; 14 and row[&#39;day&#39;] &lt;&#x3D; 20 and row[&#39;weekday&#39;] &#x3D;&#x3D; 6) or (row[&#39;month&#39;]&#x3D;&#x3D;5 and row[&#39;day&#39;] &gt;&#x3D; 24 and row[&#39;day&#39;] &lt;&#x3D; 30 and row[&#39;weekday&#39;] &#x3D;&#x3D; 6) or ((row[&#39;month&#39;]&#x3D;&#x3D;9 and row[&#39;day&#39;] &gt;&#x3D; 1 and row[&#39;day&#39;] &lt;&#x3D; 6) or (row[&#39;month&#39;]&#x3D;&#x3D;8 and row[&#39;day&#39;] &#x3D;&#x3D; 31) and row[&#39;weekday&#39;] &#x3D;&#x3D; 6) or (row[&#39;month&#39;]&#x3D;&#x3D;10 and row[&#39;day&#39;] &gt;&#x3D; 7 and row[&#39;day&#39;] &lt;&#x3D; 13 and row[&#39;weekday&#39;] &#x3D;&#x3D; 6) or (row[&#39;month&#39;]&#x3D;&#x3D;11 and row[&#39;day&#39;] &gt;&#x3D; 21 and row[&#39;day&#39;] &lt;&#x3D; 27 and row[&#39;weekday&#39;] &#x3D;&#x3D; 2) else 0, axis&#x3D;1) df[&#39;store_and_fwd_flag&#39;] &#x3D; df[&#39;store_and_fwd_flag&#39;].map(lambda x: 0 if x &#x3D;&#x3D;&#39;N&#39; else 1)extract_features(train)extract_features(test) 123456789101112131415161718192021train &#x3D; train.drop([&#39;id&#39;,&#39;pickup_datetime&#39;,&#39;dropoff_datetime&#39;],axis&#x3D;1)test &#x3D; test.drop([&#39;id&#39;,&#39;pickup_datetime&#39;],axis&#x3D;1) train[&#39;trip_duration&#39;] &#x3D; np.log(train[&#39;trip_duration&#39;].values + 1)train &#x3D; pd.get_dummies(train)test &#x3D; pd.get_dummies(test)train &#x3D; pd.get_dummies(train)test &#x3D; pd.get_dummies(test)from sklearn.model_selection import train_test_splitTrain, Test &#x3D; train_test_split(train[0:100000], test_size &#x3D; 0.2)x_train &#x3D; Train.drop([&#39;trip_duration&#39;], axis&#x3D;1)y_train &#x3D; Train[&quot;trip_duration&quot;]x_val &#x3D; Test.drop([&#39;trip_duration&#39;], axis&#x3D;1)y_val &#x3D; Test[&quot;trip_duration&quot;]y_val &#x3D; y_val.reset_index().drop(&#39;index&#39;,axis &#x3D; 1)y_train &#x3D; y_train.reset_index().drop(&#39;index&#39;,axis &#x3D; 1)X_test &#x3D; test 建模预测 12345678import lightgbm as lgbimport xgboost as xgbfrom sklearn.model_selection import GridSearchCV,cross_val_scorefrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressorfrom sklearn.model_selection import cross_val_score, train_test_splitfrom sklearn.metrics import mean_squared_error, mean_absolute_errorfrom sklearn import linear_modelfrom sklearn import preprocessing 12345678910111213141516171819202122232425262728293031def build_model_lr(x_train,y_train): reg_model &#x3D; linear_model.LinearRegression() reg_model.fit(x_train,y_train) return reg_modeldef build_model_gbdt(x_train,y_train): estimator &#x3D;GradientBoostingRegressor(loss&#x3D;&#39;ls&#39;,subsample&#x3D; 0.85,max_depth&#x3D; 5,n_estimators &#x3D; 100) param_grid &#x3D; &#123; &#39;learning_rate&#39;: [0.05,0.08,0.1,0.2], &#125; gbdt &#x3D; GridSearchCV(estimator, param_grid,cv&#x3D;3) gbdt.fit(x_train,y_train) print(gbdt.best_params_) # print(gbdt.best_estimator_ ) return gbdtdef build_model_xgb(x_train,y_train): model &#x3D; xgb.XGBRegressor(n_estimators&#x3D;120, learning_rate&#x3D;0.08, gamma&#x3D;0, subsample&#x3D;0.8,\\ colsample_bytree&#x3D;0.9, max_depth&#x3D;5) #, objective &#x3D;&#39;reg:squarederror&#39; model.fit(x_train, y_train) return modeldef build_model_lgb(x_train,y_train): estimator &#x3D; lgb.LGBMRegressor(num_leaves&#x3D;63,n_estimators &#x3D; 100) param_grid &#x3D; &#123; &#39;learning_rate&#39;: [0.01, 0.05, 0.1], &#125; gbm &#x3D; GridSearchCV(estimator, param_grid) gbm.fit(x_train, y_train) return gbm 1234567891011121314print(&#39;predict gbdt...&#39;)model_gbdt &#x3D; build_model_gbdt(x_train,y_train)val_gbdt &#x3D; model_gbdt.predict(x_val)subA_gbdt &#x3D; model_gbdt.predict(X_test)print(&#39;predict XGB...&#39;)model_xgb &#x3D; build_model_xgb(x_train,y_train)val_xgb &#x3D; model_xgb.predict(x_val)subA_xgb &#x3D; model_xgb.predict(X_test)print(&#39;predict lgb...&#39;)model_lgb &#x3D; build_model_lgb(x_train,y_train)val_lgb &#x3D; model_lgb.predict(x_val)subA_lgb &#x3D; model_lgb.predict(X_test) 12345678910111213141516171819202122232425262728293031323334353637# Stacking## 第一层train_lgb_pred &#x3D; model_lgb.predict(x_train)train_xgb_pred &#x3D; model_xgb.predict(x_train)train_gbdt_pred &#x3D; model_gbdt.predict(x_train)Strak_X_train &#x3D; pd.DataFrame()Strak_X_train[&#39;Method_1&#39;] &#x3D; train_lgb_predStrak_X_train[&#39;Method_2&#39;] &#x3D; train_xgb_predStrak_X_train[&#39;Method_3&#39;] &#x3D; train_gbdt_predStrak_X_val &#x3D; pd.DataFrame()Strak_X_val[&#39;Method_1&#39;] &#x3D; val_lgbStrak_X_val[&#39;Method_2&#39;] &#x3D; val_xgbStrak_X_val[&#39;Method_3&#39;] &#x3D; val_gbdtStrak_X_test &#x3D; pd.DataFrame()Strak_X_test[&#39;Method_1&#39;] &#x3D; subA_lgbStrak_X_test[&#39;Method_2&#39;] &#x3D; subA_xgbStrak_X_test[&#39;Method_3&#39;] &#x3D; subA_gbdt## level2-method model_lr_Stacking &#x3D; build_model_lr(Strak_X_train,y_train)## 训练集train_pre_Stacking &#x3D; model_lr_Stacking.predict(Strak_X_train)print(&#39;RMSE of Stacking-LR:&#39;,sqrt(mean_squared_error(y_train,train_pre_Stacking)))## 验证集val_pre_Stacking &#x3D; model_lr_Stacking.predict(Strak_X_val)print(&#39;RMSE of Stacking-LR:&#39;,sqrt(mean_squared_error(y_val,val_pre_Stacking)))## 预测集print(&#39;Predict Stacking-LR...&#39;)subA_Stacking &#x3D; model_lr_Stacking.predict(Strak_X_test) 生成预测 123456sub &#x3D; pd.DataFrame()Test_data &#x3D; pd.read_csv(&#39;C:\\\\Users\\\\18438\\\\Desktop\\\\NYC_TAXI\\\\test.csv&#39;, sep&#x3D;&#39;,&#39;)sub[&#39;id&#39;] &#x3D; Test_data[&#39;id&#39;]sub[&#39;trip_duration&#39;] &#x3D; subA_Stackingsub.to_csv(&#39;C:\\\\Users\\\\18438\\\\Desktop\\\\NYC_TAXI\\\\sub_Stacking.csv&#39;,index&#x3D;False)","categories":[],"tags":[]},{"title":"天池-二手车交易价格预测","slug":"天池-二手车交易价格预测","date":"2020-04-25T12:45:32.000Z","updated":"2020-04-29T01:59:25.912Z","comments":true,"path":"2020/04/25/天池-二手车交易价格预测/","link":"","permalink":"http://yoursite.com/2020/04/25/%E5%A4%A9%E6%B1%A0-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/","excerpt":"","text":"赛题背景：赛题以预测二手车的交易价格为任务，数据集报名后可见并可下载，该数据来自某交易平台的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量。为了保证比赛的公平性，将会从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时会对name、model、brand和regionCode等信息进行脱敏。主要工作分为以下几个方面：1 数据清洗，去除异常点，无用的变量，对空缺值进行删除或填充2 探索性数据分析，主要观察分布不均匀的特征，异常点以及发现特征和标签之间的相关性3 特征工程 ，根据前面的分析对特征进行加工，增强特征，特征筛选4 建模预测，使用LIGHTGBM对数据进行十折交叉验证 数据清洗 123456789import pandas as pdimport numpy as np import matplotlib.pyplot as plt%matplotlib inlineimport seaborn as snstrain &#x3D; pd.read_csv(&#39;train.csv&#39;,sep&#x3D; &#39; &#39;)print(&#39;train shape:&#39;,train.shape)test &#x3D; pd.read_csv(&#39;test.csv&#39;,sep&#x3D; &#39; &#39;)print(&#39;test shape:&#39;,test.shape) 对有空缺值的数据先以众数填充（因为这些变量都为分类变量） 12345678910train[&#39;model&#39;] &#x3D; train[&#39;model&#39;].fillna(0)train[&#39;bodyType&#39;] &#x3D; train[&#39;bodyType&#39;].fillna(0)train[&#39;fuelType&#39;] &#x3D; train[&#39;fuelType&#39;].fillna(0)train[&#39;gearbox&#39;] &#x3D; train[&#39;gearbox&#39;].fillna(0)test[&#39;model&#39;] &#x3D; test[&#39;model&#39;].fillna(0)test[&#39;bodyType&#39;] &#x3D; test[&#39;bodyType&#39;].fillna(0)test[&#39;fuelType&#39;] &#x3D; test[&#39;fuelType&#39;].fillna(0)test[&#39;gearbox&#39;] &#x3D; test[&#39;gearbox&#39;].fillna(0) 对各个特征进行统计计数，发现seller,offertype变量存在严重的不平衡性，故删去。notRepairedDamage特征存在 - 值，我们先用nan填充。 1234train.drop([&#39;seller&#39;,&#39;offerType&#39;],axis&#x3D;1,inplace&#x3D;True)test.drop([&#39;seller&#39;,&#39;offerType&#39;],axis&#x3D;1,inplace&#x3D;True)train[&#39;notRepairedDamage&#39;].replace(&#39;-&#39;,&#39;np.nan&#39;,inplace&#x3D;True)test[&#39;notRepairedDamage&#39;].replace(&#39;-&#39;,&#39;np.nan&#39;,inplace&#x3D;True) 定义去除异常点的函数(利用箱线图，认为低于QL-3IQR或高于QU+3IQR的数据点是异常点) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def outliers_proc(data, col_name, scale&#x3D;3): &quot;&quot;&quot; 用于清洗异常值，默认用 box_plot（scale&#x3D;3）进行清洗 :param data: 接收 pandas 数据格式 :param col_name: pandas 列名 :param scale: 尺度 :return: &quot;&quot;&quot; def box_plot_outliers(data_ser, box_scale): &quot;&quot;&quot; 利用箱线图去除异常值 :param data_ser: 接收 pandas.Series 数据格式 :param box_scale: 箱线图尺度， :return: &quot;&quot;&quot; iqr &#x3D; box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) #得到IQR val_low &#x3D; data_ser.quantile(0.25) - iqr #得到下限 这里规定QL-(几倍的)IQR val_up &#x3D; data_ser.quantile(0.75) + iqr #上限 QU+（几倍的）IQR rule_low &#x3D; (data_ser &lt; val_low) rule_up &#x3D; (data_ser &gt; val_up) return (rule_low, rule_up), (val_low, val_up) data_n &#x3D; data.copy() data_series &#x3D; data_n[col_name] rule, value &#x3D; box_plot_outliers(data_series, box_scale&#x3D;scale) index &#x3D; np.arange(data_series.shape[0])[rule[0] | rule[1]] print(&quot;Delete number is: &#123;&#125;&quot;.format(len(index))) data_n &#x3D; data_n.drop(index) data_n.reset_index(drop&#x3D;True, inplace&#x3D;True) print(&quot;Now column number is: &#123;&#125;&quot;.format(data_n.shape[0])) index_low &#x3D; np.arange(data_series.shape[0])[rule[0]] outliers &#x3D; data_series.iloc[index_low] print(&quot;Description of data less than the lower bound is:&quot;) print(pd.Series(outliers).describe()) index_up &#x3D; np.arange(data_series.shape[0])[rule[1]] outliers &#x3D; data_series.iloc[index_up] print(&quot;Description of data larger than the upper bound is:&quot;) print(pd.Series(outliers).describe()) fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(10, 7)) sns.boxplot(y&#x3D;data[col_name], data&#x3D;data, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[0]) sns.boxplot(y&#x3D;data_n[col_name], data&#x3D;data_n, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[1]) return data_ntrain &#x3D; outliers_proc(train, &#39;power&#39;, scale&#x3D;3)train &#x3D; outliers_proc(train, &#39;model&#39;, scale&#x3D;3)train &#x3D; outliers_proc(train, &#39;fuelType&#39;, scale&#x3D;3) 去除异常点前后的箱线图对比： 解决标签的长尾分布问题 12345678train[&#39;price&#39;] &#x3D; np.log1p(train[&#39;price&#39;])# 合并方便后面的操作df &#x3D; pd.concat([train, test], ignore_index&#x3D;True)#题目规定了范围的power(0~600)。处理一下df[&#39;power&#39;] &#x3D; df[&#39;power&#39;].map(lambda x: 600 if x&gt;600 else x) 探索性数据分析这一步应该和数据清洗，特征工程相辅相成，前面的代码有不少思路来源于EDA。主要从以下几个方面来分析数据： 1 缺失 2 变量分布 3 相关关系代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#统计缺失值trainmissing &#x3D; train.isnull().sum()trainmissing[trainmissing &gt; 0].sort_values().plot.bar()testmissing &#x3D; test.isnull().sum()testmissing[testmissing &gt; 0].sort_values().plot.bar()#观察定性变量的分布cols &#x3D; [&#39;model&#39;,&#39;brand&#39;,&#39;bodyType&#39;,&#39;fuelType&#39;,&#39;gearbox&#39;,&#39;notRepairedDamage&#39;,&#39;regionCode&#39;,&#39;seller&#39;,&#39;offerType&#39;]for col in cols: s &#x3D; train[col].value_counts() print(col) print(s) print(&#39;-----------------------------------&#39;)#做出一些特征的箱线图cols &#x3D; [&#39;model&#39;,&#39;brand&#39;,&#39;bodyType&#39;,&#39;fuelType&#39;,&#39;gearbox&#39;,&#39;notRepairedDamage&#39;]for c in cols: train[c] &#x3D; train[c].astype(&#39;category&#39;)def boxplot(x,y,**kwargs): sns.boxplot(x&#x3D;x,y&#x3D;y) x &#x3D; plt.xticks(rotation&#x3D;90) f &#x3D; pd.melt(train,id_vars&#x3D;[&#39;price&#39;],value_vars &#x3D; cols)g &#x3D; sns.FacetGrid(f,col&#x3D;&#39;variable&#39;,col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)g &#x3D; g.map(boxplot,&#39;value&#39;,&#39;price&#39;)#观察定量变量的分布number &#x3D; [&#39;power&#39;,&#39;kilometer&#39;,&#39;v_0&#39;,&#39;v_1&#39;,&#39;v_2&#39;,&#39;v_3&#39;,&#39;v_4&#39;,&#39;v_5&#39;,&#39;v_6&#39;,&#39;v_7&#39;,&#39;v_8&#39;,&#39;v_9&#39;,&#39;v_10&#39;,&#39;v_11&#39;,&#39;v_12&#39;,&#39;v_13&#39;,&#39;v_14&#39;,&#39;price&#39;]for col in number: n &#x3D; train[col].value_counts() print(col) print(n) print(&#39;-----------------------------------&#39;)#对可读性差的定量变量进行相关性分析corr &#x3D; train[number].corr()corr[&#39;price&#39;].sort_values()plt.figure(figsize&#x3D;(10,8))sns.heatmap(corr,square&#x3D;True,vmax&#x3D;0.8)#预测值的分布train[&#39;price&#39;].hist() &lt;&lt;特征工程提取时间，地区 123456789101112131415161718192021222324252627282930from datetime import datetimedef date_process(x): year &#x3D; int(str(x)[:4]) month &#x3D; int(str(x)[4:6]) day &#x3D; int(str(x)[6:8]) if month &lt; 1: month &#x3D; 1 date &#x3D; datetime(year, month, day) return datedf[&#39;regDate&#39;] &#x3D; df[&#39;regDate&#39;].apply(date_process)df[&#39;creatDate&#39;] &#x3D; df[&#39;creatDate&#39;].apply(date_process)df[&#39;regDate_year&#39;] &#x3D; df[&#39;regDate&#39;].dt.yeardf[&#39;regDate_month&#39;] &#x3D; df[&#39;regDate&#39;].dt.monthdf[&#39;regDate_day&#39;] &#x3D; df[&#39;regDate&#39;].dt.daydf[&#39;creatDate_year&#39;] &#x3D; df[&#39;creatDate&#39;].dt.yeardf[&#39;creatDate_month&#39;] &#x3D; df[&#39;creatDate&#39;].dt.monthdf[&#39;creatDate_day&#39;] &#x3D; df[&#39;creatDate&#39;].dt.daydf[&#39;car_age_day&#39;] &#x3D; (df[&#39;creatDate&#39;] - df[&#39;regDate&#39;]).dt.daysdf[&#39;car_age_year&#39;] &#x3D; round(df[&#39;car_age_day&#39;] &#x2F; 365, 1)#地区df[&#39;regionCode_count&#39;] &#x3D; df.groupby([&#39;regionCode&#39;])[&#39;SaleID&#39;].transform(&#39;count&#39;)df[&#39;city&#39;] &#x3D; df[&#39;regionCode&#39;].apply(lambda x : str(x)[:2]) 对可分类的连续特征进行分桶，kilometer是已经分桶了 1234567bin &#x3D; [i*10 for i in range(31)]df[&#39;power_bin&#39;] &#x3D; pd.cut(df[&#39;power&#39;], bin, labels&#x3D;False)tong &#x3D; df[[&#39;power_bin&#39;, &#39;power&#39;]].head()bin &#x3D; [i*10 for i in range(24)]df[&#39;model_bin&#39;] &#x3D; pd.cut(df[&#39;model&#39;], bin, labels&#x3D;False)tong &#x3D; df[[&#39;model_bin&#39;, &#39;model&#39;]].head() 将稍微取值多一点的分类特征与price进行特征组合,聚类分析 12345678910111213141516171819Train_gb &#x3D; Train_data.groupby(&quot;regionCode&quot;)all_info &#x3D; &#123;&#125;for kind, kind_data in Train_gb: info &#x3D; &#123;&#125; kind_data &#x3D; kind_data[kind_data[&#39;price&#39;] &gt; 0] info[&#39;regionCode_amount&#39;] &#x3D; len(kind_data) info[&#39;regionCode_price_max&#39;] &#x3D; kind_data.price.max() info[&#39;regionCode_price_median&#39;] &#x3D; kind_data.price.median() info[&#39;regionCode_price_min&#39;] &#x3D; kind_data.price.min() info[&#39;regionCode_price_sum&#39;] &#x3D; kind_data.price.sum() info[&#39;regionCode_price_std&#39;] &#x3D; kind_data.price.std() info[&#39;regionCode_price_mean&#39;] &#x3D; kind_data.price.mean() info[&#39;regionCode_price_skew&#39;] &#x3D; kind_data.price.skew() info[&#39;regionCode_price_kurt&#39;] &#x3D; kind_data.price.kurt() info[&#39;regionCode_mad&#39;] &#x3D; kind_data.price.mad() all_info[kind] &#x3D; infobrand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;: &quot;regionCode&quot;&#125;)df &#x3D; df.merge(brand_fe, how&#x3D;&#39;left&#39;, on&#x3D;&#39;regionCode&#39;) 1234567891011121314151617181920Train_gb &#x3D; Train_data.groupby(&quot;brand&quot;)all_info &#x3D; &#123;&#125;for kind, kind_data in Train_gb: info &#x3D; &#123;&#125; kind_data &#x3D; kind_data[kind_data[&#39;price&#39;] &gt; 0] info[&#39;brand_amount&#39;] &#x3D; len(kind_data) info[&#39;brand_price_max&#39;] &#x3D; kind_data.price.max() info[&#39;brand_price_median&#39;] &#x3D; kind_data.price.median() info[&#39;brand_price_min&#39;] &#x3D; kind_data.price.min() info[&#39;brand_price_sum&#39;] &#x3D; kind_data.price.sum() info[&#39;brand_price_std&#39;] &#x3D; kind_data.price.std() info[&#39;brand_price_mean&#39;] &#x3D; kind_data.price.mean() info[&#39;brand_price_skew&#39;] &#x3D; kind_data.price.skew() info[&#39;brand_price_kurt&#39;] &#x3D; kind_data.price.kurt() info[&#39;brand_price_mad&#39;] &#x3D; kind_data.price.mad() all_info[kind] &#x3D; infobrand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;: &quot;brand&quot;&#125;)df &#x3D; df.merge(brand_fe, how&#x3D;&#39;left&#39;, on&#x3D;&#39;brand&#39;) 12345678910111213141516171819Train_gb &#x3D; Train_data.groupby(&quot;model&quot;)all_info &#x3D; &#123;&#125;for kind, kind_data in Train_gb: info &#x3D; &#123;&#125; kind_data &#x3D; kind_data[kind_data[&#39;price&#39;] &gt; 0] info[&#39;model_amount&#39;] &#x3D; len(kind_data) info[&#39;model_price_max&#39;] &#x3D; kind_data.price.max() info[&#39;model_price_median&#39;] &#x3D; kind_data.price.median() info[&#39;model_price_min&#39;] &#x3D; kind_data.price.min() info[&#39;model_price_sum&#39;] &#x3D; kind_data.price.sum() info[&#39;model_price_std&#39;] &#x3D; kind_data.price.std() info[&#39;model_price_mean&#39;] &#x3D; kind_data.price.mean() info[&#39;model_price_skew&#39;] &#x3D; kind_data.price.skew() info[&#39;model_price_kurt&#39;] &#x3D; kind_data.price.kurt() info[&#39;model_price_mad&#39;] &#x3D; kind_data.price.mad() all_info[kind] &#x3D; infobrand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;: &quot;model&quot;&#125;)df &#x3D; df.merge(brand_fe, how&#x3D;&#39;left&#39;, on&#x3D;&#39;model&#39;) 1234567891011121314151617181920Train_gb &#x3D; Train_data.groupby(&quot;kilometer&quot;)all_info &#x3D; &#123;&#125;for kind, kind_data in Train_gb: info &#x3D; &#123;&#125; kind_data &#x3D; kind_data[kind_data[&#39;price&#39;] &gt; 0] info[&#39;kilometer_amount&#39;] &#x3D; len(kind_data) info[&#39;kilometer_price_max&#39;] &#x3D; kind_data.price.max() info[&#39;kilometer_price_median&#39;] &#x3D; kind_data.price.median() info[&#39;kilometer_price_min&#39;] &#x3D; kind_data.price.min() info[&#39;kilometer_price_sum&#39;] &#x3D; kind_data.price.sum() info[&#39;kilometer_price_std&#39;] &#x3D; kind_data.price.std() info[&#39;kilometer_price_mean&#39;] &#x3D; kind_data.price.mean() info[&#39;kilometer_price_skew&#39;] &#x3D; kind_data.price.skew() info[&#39;kilometer_price_kurt&#39;] &#x3D; kind_data.price.kurt() info[&#39;kilometer_price_mad&#39;] &#x3D; kind_data.price.mad() all_info[kind] &#x3D; infobrand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;: &quot;kilometer&quot;&#125;)df &#x3D; df.merge(brand_fe, how&#x3D;&#39;left&#39;, on&#x3D;&#39;kilometer&#39;) 1234567891011121314151617181920Train_gb &#x3D; Train_data.groupby(&quot;bodyType&quot;)all_info &#x3D; &#123;&#125;for kind, kind_data in Train_gb: info &#x3D; &#123;&#125; kind_data &#x3D; kind_data[kind_data[&#39;price&#39;] &gt; 0] info[&#39;bodyType_amount&#39;] &#x3D; len(kind_data) info[&#39;bodyType_price_max&#39;] &#x3D; kind_data.price.max() info[&#39;bodyType_price_median&#39;] &#x3D; kind_data.price.median() info[&#39;bodyType_price_min&#39;] &#x3D; kind_data.price.min() info[&#39;bodyType_price_sum&#39;] &#x3D; kind_data.price.sum() info[&#39;bodyType_price_std&#39;] &#x3D; kind_data.price.std() info[&#39;bodyType_price_mean&#39;] &#x3D; kind_data.price.mean() info[&#39;bodyType_price_skew&#39;] &#x3D; kind_data.price.skew() info[&#39;bodyType_price_kurt&#39;] &#x3D; kind_data.price.kurt() info[&#39;bodyType_price_mad&#39;] &#x3D; kind_data.price.mad() all_info[kind] &#x3D; infobrand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;: &quot;bodyType&quot;&#125;)df &#x3D; df.merge(brand_fe, how&#x3D;&#39;left&#39;, on&#x3D;&#39;bodyType&#39;) 1234567891011121314151617181920Train_gb &#x3D; Train_data.groupby(&quot;fuelType&quot;)all_info &#x3D; &#123;&#125;for kind, kind_data in Train_gb: info &#x3D; &#123;&#125; kind_data &#x3D; kind_data[kind_data[&#39;price&#39;] &gt; 0] info[&#39;fuelType_amount&#39;] &#x3D; len(kind_data) info[&#39;fuelType_price_max&#39;] &#x3D; kind_data.price.max() info[&#39;fuelType_price_median&#39;] &#x3D; kind_data.price.median() info[&#39;fuelType_price_min&#39;] &#x3D; kind_data.price.min() info[&#39;fuelType_price_sum&#39;] &#x3D; kind_data.price.sum() info[&#39;fuelType_price_std&#39;] &#x3D; kind_data.price.std() info[&#39;fuelType_price_mean&#39;] &#x3D; kind_data.price.mean() info[&#39;fuelType_price_skew&#39;] &#x3D; kind_data.price.skew() info[&#39;fuelType_price_kurt&#39;] &#x3D; kind_data.price.kurt() info[&#39;fuelType_price_mad&#39;] &#x3D; kind_data.price.mad() all_info[kind] &#x3D; infobrand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;: &quot;fuelType&quot;&#125;)df &#x3D; df.merge(brand_fe, how&#x3D;&#39;left&#39;, on&#x3D;&#39;fuelType&#39;) 从使用年限，使用里程角度新建特征（使用年限越长，里程越多，二手车的折扣越大） 1234567891011121314151617181920212223242526272829303132333435# 使用年限折旧def depreciation_year1(year): if year &lt;&#x3D; 3: return 1 - year * 0.15 elif year &gt; 3 and year &lt;&#x3D; 7: return 0.55 - (year-3) * 0.1 elif year &gt; 7 and year &lt;&#x3D; 10: return 0.25 - (year-7) * 0.05 else: return 0def depreciation_year2(year): if year &lt;&#x3D; 3: return 1 - 0.85 * year * 0.11 elif year &gt; 3 and year &lt;&#x3D; 7: return 0.7195 - 0.85 * (year-3) * 0.1 elif year &gt; 7 and year &lt;&#x3D; 10: return 0.3795 - 0.85 * (year-7) * 0.09 else: return 0.15 #行驶里程def depreciation_kilometer(kilo): if kilo &lt;&#x3D; 6: return 1 - kilo * 5 &#x2F; 90 elif kilo &gt; 6 and kilo &lt;&#x3D; 12: return 0.66667 - (kilo-6) * 4 &#x2F; 90 elif kilo &gt; 12 and kilo &lt;&#x3D; 18: return 0.4 - (kilo-12) * 3 &#x2F; 90 elif kilo &gt; 18 and kilo &lt;&#x3D; 24: return 0.2 - (kilo-18) * 2 &#x2F; 90 elif kilo &gt; 24 and kilo &lt;&#x3D; 30: return 0.06667 - (kilo-24) * 1 &#x2F; 90 12345678910#提取车的使用时间df[&#39;used_time_day&#39;] &#x3D; (pd.to_datetime(df[&#39;creatDate&#39;], format&#x3D;&#39;%Y%m%d&#39;, errors&#x3D;&#39;coerce&#39;) - pd.to_datetime(df[&#39;regDate&#39;], format&#x3D;&#39;%Y%m%d&#39;, errors&#x3D;&#39;coerce&#39;)).dt.daysdf[&#39;used_time_month&#39;] &#x3D; round(df[&#39;used_time_day&#39;] &#x2F; 30)df[&#39;used_time_year&#39;] &#x3D; round(df[&#39;used_time_day&#39;] &#x2F; 365)#提取使用年限折扣 使用里程折扣df[&#39;depreciation_year1&#39;] &#x3D; df[&#39;used_time_year&#39;].apply(lambda x: depreciation_year1(x))df[&#39;depreciation_year2&#39;] &#x3D; df[&#39;used_time_year&#39;].apply(lambda x: depreciation_year2(x))df[&#39;depreciation_kilo&#39;] &#x3D; df[&#39;kilometer&#39;].apply(lambda x: depreciation_kilometer(x)) 补充的特征工程主要是对匿名特征和几个重要度较高的分类特征进行特征交叉 12345678910111213141516171819202122232425262728#第一批特征工程for i in range(15): for j in range(15): df[&#39;new&#39;+str(i)+&#39;*&#39;+str(j)]&#x3D;df[&#39;v_&#39;+str(i)]*df[&#39;v_&#39;+str(j)]#第二批特征工程for i in range(15): for j in range(15): df[&#39;new&#39;+str(i)+&#39;+&#39;+str(j)]&#x3D;df[&#39;v_&#39;+str(i)]+df[&#39;v_&#39;+str(j)]# 第三批特征工程for i in range(15): df[&#39;new&#39; + str(i) + &#39;*power&#39;] &#x3D; df[&#39;v_&#39; + str(i)] * df[&#39;power&#39;]for i in range(15): df[&#39;new&#39; + str(i) + &#39;*day&#39;] &#x3D; df[&#39;v_&#39; + str(i)] * df[&#39;car_age_day&#39;]for i in range(15): df[&#39;new&#39; + str(i) + &#39;*year&#39;] &#x3D; df[&#39;v_&#39; + str(i)] * df[&#39;car_age_year&#39;]#第四批特征工程for i in range(15): for j in range(15): df[&#39;new&#39;+str(i)+&#39;-&#39;+str(j)]&#x3D;df[&#39;v_&#39;+str(i)]-df[&#39;v_&#39;+str(j)] 五、筛选特征 123456789101112131415161718192021222324252627282930numerical_cols &#x3D; df.select_dtypes(exclude&#x3D;&#39;object&#39;).columnslist_tree &#x3D; [ &#39;model_power_sum&#39;,&#39;price&#39;,&#39;SaleID&#39;, &#39;model_power_std&#39;, &#39;model_power_median&#39;, &#39;model_power_max&#39;, &#39;brand_price_max&#39;, &#39;brand_price_median&#39;, &#39;brand_price_sum&#39;, &#39;brand_price_std&#39;, &#39;model_days_sum&#39;, &#39;model_days_std&#39;, &#39;model_days_median&#39;, &#39;model_days_max&#39;, &#39;model_bin&#39;, &#39;model_amount&#39;, &#39;model_price_max&#39;, &#39;model_price_median&#39;, &#39;model_price_min&#39;, &#39;model_price_sum&#39;, &#39;model_price_std&#39;, &#39;model_price_mean&#39;, &#39;bodyType&#39;, &#39;model&#39;, &#39;brand&#39;, &#39;fuelType&#39;, &#39;gearbox&#39;, &#39;power&#39;, &#39;kilometer&#39;, &#39;notRepairedDamage&#39;, &#39;v_0&#39;, &#39;v_1&#39;, &#39;v_2&#39;, &#39;v_3&#39;, &#39;v_4&#39;, &#39;v_5&#39;, &#39;v_6&#39;, &#39;v_7&#39;, &#39;v_8&#39;, &#39;v_9&#39;, &#39;v_10&#39;, &#39;v_11&#39;, &#39;v_12&#39;, &#39;v_13&#39;, &#39;v_14&#39;, &#39;name_count&#39;, &#39;regDate_year&#39;, &#39;car_age_day&#39;, &#39;car_age_year&#39;, &#39;power_bin&#39;,&#39;fuelType&#39;, &#39;gearbox&#39;, &#39;kilometer&#39;, &#39;notRepairedDamage&#39;, &#39;name_count&#39;, &#39;car_age_day&#39;, &#39;new3*3&#39;, &#39;new12*14&#39;, &#39;new2*14&#39;,&#39;new14*14&#39;]for i in range(15): for j in range(15): list_tree.append(&#39;new&#39;+str(i)+&#39;+&#39;+str(j))feature_cols &#x3D; [col for col in numerical_cols if col in list_tree]feature_cols &#x3D; [col for col in feature_cols if col not in [&#39;new14+6&#39;, &#39;new13+6&#39;, &#39;new0+12&#39;, &#39;new9+11&#39;, &#39;v_3&#39;, &#39;new11+10&#39;, &#39;new10+14&#39;, &#39;new12+4&#39;, &#39;new3+4&#39;, &#39;new11+11&#39;, &#39;new13+3&#39;, &#39;new8+1&#39;, &#39;new1+7&#39;, &#39;new11+14&#39;, &#39;new8+13&#39;, &#39;v_8&#39;, &#39;v_0&#39;, &#39;new3+5&#39;, &#39;new2+9&#39;, &#39;new9+2&#39;, &#39;new0+11&#39;, &#39;new13+7&#39;, &#39;new8+11&#39;, &#39;new5+12&#39;, &#39;new10+10&#39;, &#39;new13+8&#39;, &#39;new11+13&#39;, &#39;new7+9&#39;, &#39;v_1&#39;, &#39;new7+4&#39;, &#39;new13+4&#39;, &#39;v_7&#39;, &#39;new5+6&#39;, &#39;new7+3&#39;, &#39;new9+10&#39;, &#39;new11+12&#39;, &#39;new0+5&#39;, &#39;new4+13&#39;, &#39;new8+0&#39;, &#39;new0+7&#39;, &#39;new12+8&#39;, &#39;new10+8&#39;, &#39;new13+14&#39;, &#39;new5+7&#39;, &#39;new2+7&#39;, &#39;v_4&#39;, &#39;v_10&#39;, &#39;new4+8&#39;, &#39;new8+14&#39;, &#39;new5+9&#39;, &#39;new9+13&#39;, &#39;new2+12&#39;, &#39;new5+8&#39;, &#39;new3+12&#39;, &#39;new0+10&#39;, &#39;new9+0&#39;, &#39;new1+11&#39;, &#39;new8+4&#39;, &#39;new11+8&#39;, &#39;new1+1&#39;, &#39;new10+5&#39;, &#39;new8+2&#39;, &#39;new6+1&#39;, &#39;new2+1&#39;, &#39;new1+12&#39;, &#39;new2+5&#39;, &#39;new0+14&#39;, &#39;new4+7&#39;, &#39;new14+9&#39;, &#39;new0+2&#39;, &#39;new4+1&#39;, &#39;new7+11&#39;, &#39;new13+10&#39;, &#39;new6+3&#39;, &#39;new1+10&#39;, &#39;v_9&#39;, &#39;new3+6&#39;, &#39;new12+1&#39;, &#39;new9+3&#39;, &#39;new4+5&#39;, &#39;new12+9&#39;, &#39;new3+8&#39;, &#39;new0+8&#39;, &#39;new1+8&#39;, &#39;new1+6&#39;, &#39;new10+9&#39;, &#39;new5+4&#39;, &#39;new13+1&#39;, &#39;new3+7&#39;, &#39;new6+4&#39;, &#39;new6+7&#39;, &#39;new13+0&#39;, &#39;new1+14&#39;, &#39;new3+11&#39;, &#39;new6+8&#39;, &#39;new0+9&#39;, &#39;new2+14&#39;, &#39;new6+2&#39;, &#39;new12+12&#39;, &#39;new7+12&#39;, &#39;new12+6&#39;, &#39;new12+14&#39;, &#39;new4+10&#39;, &#39;new2+4&#39;, &#39;new6+0&#39;, &#39;new3+9&#39;, &#39;new2+8&#39;, &#39;new6+11&#39;, &#39;new3+10&#39;, &#39;new7+0&#39;, &#39;v_11&#39;, &#39;new1+3&#39;, &#39;new8+3&#39;, &#39;new12+13&#39;, &#39;new1+9&#39;, &#39;new10+13&#39;, &#39;new5+10&#39;, &#39;new2+2&#39;, &#39;new6+9&#39;, &#39;new7+10&#39;, &#39;new0+0&#39;, &#39;new11+7&#39;, &#39;new2+13&#39;, &#39;new11+1&#39;, &#39;new5+11&#39;, &#39;new4+6&#39;, &#39;new12+2&#39;, &#39;new4+4&#39;, &#39;new6+14&#39;, &#39;new0+1&#39;, &#39;new4+14&#39;, &#39;v_5&#39;, &#39;new4+11&#39;, &#39;v_6&#39;, &#39;new0+4&#39;, &#39;new1+5&#39;, &#39;new3+14&#39;, &#39;new2+10&#39;, &#39;new9+4&#39;, &#39;new2+6&#39;, &#39;new14+14&#39;, &#39;new11+6&#39;, &#39;new9+1&#39;, &#39;new3+13&#39;, &#39;new13+13&#39;, &#39;new10+6&#39;, &#39;new2+3&#39;, &#39;new2+11&#39;, &#39;new1+4&#39;, &#39;v_2&#39;, &#39;new5+13&#39;, &#39;new4+2&#39;, &#39;new0+6&#39;, &#39;new7+13&#39;, &#39;new8+9&#39;, &#39;new9+12&#39;, &#39;new0+13&#39;, &#39;new10+12&#39;, &#39;new5+14&#39;, &#39;new6+10&#39;, &#39;new10+7&#39;, &#39;v_13&#39;, &#39;new5+2&#39;, &#39;new6+13&#39;, &#39;new9+14&#39;, &#39;new13+9&#39;, &#39;new14+7&#39;, &#39;new8+12&#39;, &#39;new3+3&#39;, &#39;new6+12&#39;, &#39;v_12&#39;, &#39;new14+4&#39;, &#39;new11+9&#39;, &#39;new12+7&#39;, &#39;new4+9&#39;, &#39;new4+12&#39;, &#39;new1+13&#39;, &#39;new0+3&#39;, &#39;new8+10&#39;, &#39;new13+11&#39;, &#39;new7+8&#39;, &#39;new7+14&#39;, &#39;v_14&#39;, &#39;new10+11&#39;, &#39;new14+8&#39;, &#39;new1+2&#39;]]df &#x3D; df[feature_cols] 导出数据 1234567tree_data &#x3D; dfprint(tree_data.shape)train_num &#x3D; df.shape[0]-50000tree_data[0:int(train_num)].to_csv(&#39;train_tree.csv&#39;, index&#x3D;0,sep&#x3D;&#39; &#39;)tree_data[train_num:train_num+50000].to_csv(&#39;text_tree.csv&#39;, index&#x3D;0,sep&#x3D;&#39; &#39;)print(&#39;树模型数据已经准备完毕~~~~~~~&#39;) 建模预测 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566Train_data &#x3D; pd.read_csv(&#39;train_tree.csv&#39;, sep&#x3D;&#39; &#39;)TestA_data &#x3D; pd.read_csv(&#39;text_tree.csv&#39;, sep&#x3D;&#39; &#39;)numerical_cols &#x3D; Train_data.columnsfeature_cols &#x3D; [col for col in numerical_cols if col not in [&#39;price&#39;,&#39;SaleID&#39;]]#提前特征列，标签列构造训练样本和测试样本X_data &#x3D; Train_data[feature_cols]X_test &#x3D; TestA_data[feature_cols]print(X_data.shape)print(X_test.shape)X_data &#x3D; np.array(X_data)X_test &#x3D; np.array(X_test)Y_data &#x3D; np.array(Train_data[&#39;price&#39;])&quot;&quot;&quot;lightgbm&quot;&quot;&quot;#自定义损失函数def myFeval(preds, xgbtrain): label &#x3D; xgbtrain.get_label() score &#x3D; mean_absolute_error(np.expm1(label), np.expm1(preds)) return &#39;myFeval&#39;, score, Falseparam &#x3D; &#123;&#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;num_leaves&#39;: 31, &#39;max_depth&#39;: -1, &quot;lambda_l2&quot;: 2, # 防止过拟合 &#39;min_data_in_leaf&#39;: 20, # 防止过拟合，好像都不用怎么调 &#39;objective&#39;: &#39;regression_l1&#39;, &#39;learning_rate&#39;: 0.01, &quot;min_child_samples&quot;: 20, &quot;feature_fraction&quot;: 0.8, &quot;bagging_freq&quot;: 1, &quot;bagging_fraction&quot;: 0.8, &quot;bagging_seed&quot;: 11, &quot;metric&quot;: &#39;mae&#39;, &#125;folds &#x3D; KFold(n_splits&#x3D;10, shuffle&#x3D;True, random_state&#x3D;2018)oof_lgb &#x3D; np.zeros(len(X_data))predictions_lgb &#x3D; np.zeros(len(X_test))predictions_train_lgb &#x3D; np.zeros(len(X_data))for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_data, Y_data)): print(&quot;fold n°&#123;&#125;&quot;.format(fold_ + 1)) trn_data &#x3D; lgb.Dataset(X_data[trn_idx], Y_data[trn_idx]) val_data &#x3D; lgb.Dataset(X_data[val_idx], Y_data[val_idx]) num_round &#x3D; 100000000 clf &#x3D; lgb.train(param, trn_data, num_round, valid_sets&#x3D;[trn_data, val_data], verbose_eval&#x3D;300, early_stopping_rounds&#x3D;600, feval &#x3D; myFeval) oof_lgb[val_idx] &#x3D; clf.predict(X_data[val_idx], num_iteration&#x3D;clf.best_iteration) predictions_lgb +&#x3D; clf.predict(X_test, num_iteration&#x3D;clf.best_iteration) &#x2F; folds.n_splits predictions_train_lgb +&#x3D; clf.predict(X_data, num_iteration&#x3D;clf.best_iteration) &#x2F; folds.n_splitsprint(&quot;lightgbm score: &#123;:&lt;8.8f&#125;&quot;.format(mean_absolute_error(np.expm1(oof_lgb), np.expm1(Y_data)))) 测试集输出 1234567predictions &#x3D; np.exp(predictions_lgb)-1predictions[predictions &lt; 0] &#x3D; 0sub &#x3D; pd.DataFrame()sub[&#39;SaleID&#39;] &#x3D; TestA_data.SaleIDsub[&#39;price&#39;] &#x3D; predictionssub.to_csv(&#39;lgb_test.csv&#39;, index&#x3D;False) 结果：","categories":[],"tags":[]}],"categories":[],"tags":[]}